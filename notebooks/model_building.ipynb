{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e46c53",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79717e5",
   "metadata": {},
   "source": [
    "There are several top notch deep learning frameworks that would suffice for this task; but in this case we'll be using __Tensorflow__ from __Google__.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6365d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Important constants.\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f733d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with the Tensorflow data pipeline.\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    r\"C:\\Users\\ifunanyaScript\\Everything\\BrainTumour_DiagnosisApp\\data\\clean_dataset\",\n",
    "    shuffle = True,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = (BATCH_SIZE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12a670",
   "metadata": {},
   "source": [
    "The dataset is loaded as batches specified by the \"batch_size\" parameter. In this case, 3000 images batched into sizes of 32. Thus, 94 batches and that would be the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6fceb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61804885",
   "metadata": {},
   "source": [
    "When one wants to train a model with a particular dataset, the normal practice is to split said dataset into separate chunks; particularly a training chunk and a testing chunk. The purpose of splitting the data is for evaluation purposes after training the model.  \n",
    "Most times, ML folks use __scikit-learn's__ ___train_test_split___. In this case, we'll use the ___take___ and ___skip___ attributes of the dataset object which allows us to grab a portion of the dataset by batches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0528a",
   "metadata": {},
   "source": [
    "The training chunk is always a lot larger that the validation chunk and testing chunk, so that the model is trained on as much data as possible. We'll set aside for training, validation and testing; 80%, 10%, 10% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a54098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset should be 75 batches\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "print(f\"The training dataset should be {int(len(dataset)*train_size)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf83b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes first 75 batches of the dataset\n",
    "train_ds = dataset.take(75)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42538dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remaining data after the training chunk.\n",
    "remnant = dataset.skip(75)\n",
    "len(remnant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7867d73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation dataset should be 9 batches\n"
     ]
    }
   ],
   "source": [
    "val_size = 0.1\n",
    "print(f\"The validation dataset should be {int(len(dataset)*val_size)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70298112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes the first 9 batch of the remaining data.\n",
    "val_ds = remnant.take(9)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a787058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes all the data after the first 9 batches.\n",
    "test_ds = remnant.skip(9)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e42e98",
   "metadata": {},
   "source": [
    "All of these snippets can be wrapped in a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1edead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(dataset, train_split=0.8, validation_split=0.1, test_split=0.1, shuffle=True, buffer=1000):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to split a dataset into the necessary chunks and return\n",
    "    said chunks accordingly.\n",
    "    \n",
    "    A dataset is passed as an argument and the partitions are made with the predefined split sizes.\n",
    "    One can also alter the split sizes by changing the values while calling the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_batches = len(dataset)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer, seed=321)\n",
    "    \n",
    "    train_size = int(train_split*number_of_batches)\n",
    "    validation_size = int(validation_split*number_of_batches)\n",
    "    \n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(validation_size)\n",
    "    test_ds = dataset.skip(train_size).skip(validation_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a423c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset of 94 batches was chunked as follows: \n",
      "75 batches for the Training dataset, \n",
      "9 batches for the Validation dataset and, \n",
      "10 batches for the Testing dataset.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = chunking(dataset)\n",
    "print(f\"\"\"The dataset of 94 batches was chunked as follows: \n",
    "{len(train_ds)} batches for the Training dataset, \n",
    "{len(val_ds)} batches for the Validation dataset and, \n",
    "{len(test_ds)} batches for the Testing dataset.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b17a56",
   "metadata": {},
   "source": [
    "Caching is used to improve data retrieval performance by reducing the need to access underlying storage.  \n",
    "In short, the dataset is cached in memory. This reduces training time because there will be no need open files and read images during each epoch.. The next epochs will reuse the data cached by the cache transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cadbac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers for data_augmentation, resizing and rescaling.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomContrast(0.6),\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.3)\n",
    "])\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "num_classes = 2\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
